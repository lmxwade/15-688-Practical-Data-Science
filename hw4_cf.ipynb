{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "\n",
    "**THIS IS ONLY FOR 15-688 STUDENTS**\n",
    "\n",
    "In this question you will use collaborative filtering to make movie recommendations. The purpose of this question is to become familiar with recommendation systems, how they train and how they create recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering by Matrix Factorization\n",
    "\n",
    "In collaborative filtering, we take the matrix of user ratings and attempt to factorize it into user-related features and movie-related features. By viewing ratings as the product of both user and movie properties, we can predict user ratings on movies they have not yet watched.\n",
    "\n",
    "In more formal terms, given some partially filled ratings matrix $X\\in \\mathbb{R}^{m\\times n}$, we want to find feature matrices $U \\in \\mathbb{R}^{m\\times k}$ and $V \\in \\mathbb{R}^{n\\times k}$ such that $UV^T = X$. Each row of $U$ is a feature-vector corresponding to a user, and each row of $V$ is a feature-vector corresponding to a movie. $u_i^Tv_j$ is the predicted rating of user $i$ on movie $j$. This is our hypothesis function for collaborative filtering:\n",
    "\n",
    "$$h_\\theta(i,j) = u_i^T v_j$$\n",
    "\n",
    "\n",
    "$X$ is usually quite sparse, so we can indicate the absence of a rating with the value 0. Let $S$ be the set of $(i,j)$ such that $X_{i,j} \\neq 0$, so $S$ is the set of all pairs for which we have a rating. When factorizing $X$ into $U$ and $V$, we wish to minimize the squared loss between our hypothesis for $(i,j) \\in S$ and the actual value:\n",
    "\n",
    "$$\\ell\\left(h_\\theta(i,j),X_{i,j}\\right) = \\left(h_\\theta(i,j) - X_{i,j}\\right)^2\\qquad\\forall (i,j) \\in S$$\n",
    "\n",
    "The total loss is the sum of these individual losses and an additional $l_2$ penalty on the parameters (for regularization), so our total loss is:\n",
    "\n",
    "$$\\sum_{i,j\\in S}\\ell(h_\\theta(i,j),X_{i,j}) + \\lambda_u ||U| |_2^2 + \\lambda_v ||V||_2^2$$\n",
    "\n",
    "For this assignment, we'll let the regularizing term weights be $\\lambda_u = \\lambda_v = \\lambda$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy.linalg as la\n",
    "import matplotlib\n",
    "import gzip\n",
    "import csv\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "matplotlib.use(\"svg\")\n",
    "if not os.environ.get(\"DISABLE_TESTING\", False):\n",
    "    %matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from testing.testing import test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieLens rating dataset\n",
    "\n",
    "We will be using the MovieLens small dataset. The original dataset has over 27 million ratings; we will use only 100k ratings. You can read about the dataset [here](https://grouplens.org/datasets/movielens/).\n",
    "\n",
    "For this assignment, we will only be looking at the ratings data, and ignoring not their movie data and user made tags for movies) which could be used to improve the ability of the recommendation system. We begin by giving you some code to read the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(fn_ratings=\"ratings.csv.gz\", fn_movies=\"movies.csv.gz\"):\n",
    "    \"\"\"read the GZipped CSV data and split it into headers and newlines.\n",
    "\n",
    "    kwargs:\n",
    "        fn_ratings : str -- file with ratings\n",
    "        fn_movies : str -- file with movie names and ids\n",
    "    \n",
    "    returns: Tuple[ratings, movies] where\n",
    "      ratings : Tuple[np.ndarray[int], np.ndarray[int], np.ndarray[float]] -- a list of user ids, movie ids, and corresponding ratings\n",
    "      movies : Dict[int, str] -- the lookup table from movie ID to movie name\n",
    "    \"\"\"\n",
    "    \n",
    "    movies = {}\n",
    "    ratings_userid  = []\n",
    "    ratings_movieid = []\n",
    "    ratings_rating  = []\n",
    "    with gzip.open(fn_movies, 'rt', newline=\"\", encoding='utf-8') as f:\n",
    "        csvobj = csv.reader(f)\n",
    "        assert tuple(next(csvobj)) == (\"movieId\", \"title\")\n",
    "        for row in csvobj:\n",
    "            movies[int(row[0])] = row[1]\n",
    "\n",
    "    with gzip.open(fn_ratings, 'rt', newline=\"\", encoding='utf-8') as f:\n",
    "        csvobj = csv.reader(f)\n",
    "        assert tuple(next(csvobj)) == (\"userId\", \"movieId\", \"rating\")\n",
    "        for row in csvobj:\n",
    "            ratings_userid.append(int(row[0]))\n",
    "            ratings_movieid.append(int(row[1]))\n",
    "            ratings_rating.append(float(row[2]))\n",
    "\n",
    "    return (np.array(ratings_userid) - 1, np.array(ratings_movieid), np.array(ratings_rating)), movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Matrix factorization requires that we have our ratings stored in a matrix of users, so the first task is to take the dataframe and convert it into this format. Typically these matrices are extremely large and sparse (especially if you want to process the 24 million ratings), and so we work with sparse matrices here. \n",
    "\n",
    "### Specification\n",
    "\n",
    "You should produce a ratings matrix and a movie lookup dictionary, where:\n",
    "* Each row of the ratings matrix corresponds to a user. (The user IDs already form a consecutive range of numbers.)\n",
    "* Each column of the ratings matrix corresponds to a movie. The order of the columns doesn't matter. The movie IDs do not form a consecutive range of numbers, so you need to change the movie IDs to make them consecutive.\n",
    "* Any entry that does not have a rating should have a default value of 0. \n",
    "* You should produce a new movie lookup dictionary that correctly maps your updated movie IDs to the movie title. Any movies not present in the dataset should be omitted from the new lookup dictionary.\n",
    "* Split the data using [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html); you should be splitting by individual _ratings_ and not by users. (That is, different ratings from the same user are allowed to appear in the training set and test set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING process: PASSED 5/5\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def process_test(process):\n",
    "    ratings, movies_original = read_csv()\n",
    "    X_tr, X_te, movies = process(*ratings, movies_original)\n",
    "    \n",
    "    # Check format of created arrays:\n",
    "    test.equal(repr(X_tr), \"\"\"<610x9724 sparse matrix of type '<class 'numpy.float64'>'\n",
    "\twith 90752 stored elements in COOrdinate format>\"\"\")\n",
    "    test.equal(repr(X_te), \"\"\"<610x9724 sparse matrix of type '<class 'numpy.float64'>'\n",
    "\twith 10084 stored elements in COOrdinate format>\"\"\")\n",
    "\n",
    "    # Correct number of movies:\n",
    "    test.equal(len(movies), X_tr.shape[1])\n",
    "    # No new movie titles:\n",
    "    test.equal(len(set(movies) - set(movies_original.values())), 0)\n",
    "    # 18 movies that don't appear in the dataset and should be removed:\n",
    "    test.equal(len(set(movies_original.values()) - set(movies)), 18)\n",
    "\n",
    "@test\n",
    "def process(ratings_userid, ratings_movieid, ratings_rating, movies, test_size=0.1, random_state=0xCAFE):\n",
    "    \"\"\" Given rating data, split the data into training and testing sets and convert them to sparse matrices.\n",
    "\n",
    "        args: \n",
    "            ratings_userid  : np.ndarray[num_ratings] -- vector of user Ids\n",
    "            ratings_movieid : np.ndarray[num_ratings] -- vector of movie Ids\n",
    "            ratings_rating  : np.ndarray[num_ratings] -- vector of rating values\n",
    "            movies  : Dict[int, str] -- lookup from movie Id to movie name\n",
    "\n",
    "        kwargs:\n",
    "            test_size : float -- fraction of dataset to place in the test set\n",
    "            random_state : int -- the random seed for dataset splitting\n",
    "\n",
    "        return: Tuple[X_train, X_test, movies] \n",
    "            X_train : sp.coo_matrix -- the training data, as a sparse matrix\n",
    "            X_test : sp.coo_matrix -- the test data, as a sparse matrix\n",
    "            movies  : List[str] -- list of movie names, each at position equal to the new movie Id. \n",
    "    \"\"\"\n",
    "    \n",
    "    old_new = {}\n",
    "    new_movies = {}\n",
    "    count = 0\n",
    "    for old_id in ratings_movieid:\n",
    "        if (old_id not in old_new.keys()):\n",
    "            old_new[old_id] = count\n",
    "            new_movies[count] = movies[old_id]\n",
    "            count += 1\n",
    "    new_id = [old_new[old_id] for old_id in ratings_movieid]\n",
    "    ratings_newmovieid = np.asarray(new_id)\n",
    "    new_movies_list = [new_movies[i] for i in new_movies.keys()]\n",
    "    U_train, U_test, V_train, V_test, R_train, R_test = train_test_split(ratings_userid, ratings_newmovieid, ratings_rating, test_size=test_size, random_state=42)\n",
    "    \n",
    "    m = len(set(ratings_userid))\n",
    "    n = len(new_movies)\n",
    "    X_train = coo_matrix((R_train, (U_train, V_train)), shape = (m,n))\n",
    "    X_test = coo_matrix((R_test, (U_test, V_test)), shape = (m,n))\n",
    "    \n",
    "    return X_train, X_test, new_movies_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Minimization for Collaborative Filtering\n",
    "Now we build the collaborative filtering recommendation system. We will use a method known as alternating least squares. We alternate between optimizing $U$ and $V$ and holding the other constant. By treating one matrix as a constant, we get a weighted least squares problem which we can solve easily. More details can be found in the lecture notes.\n",
    "\n",
    "We begin by writing a function to calculate the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING error: PASSED 1/1\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def error_test(error):\n",
    "    U = np.array([[.1, .2, .3, .4, .5]])\n",
    "    V = np.array([[1., 2., 3., 4., 5.], [3., 4., 5., 6., 7.]])\n",
    "    X = np.array([[5., 0]])\n",
    "    \n",
    "    test.equal(error(X, U, V), 0.25)\n",
    "\n",
    "@test\n",
    "def error(X, U, V):\n",
    "    \"\"\" Compute the mean error of the observed ratings in X and their estimated values. \n",
    "\n",
    "        args: \n",
    "            X : np.array[num_users, num_movies] -- the ratings matrix\n",
    "            U : np.array[num_users, num_features] -- a matrix of features for each user\n",
    "            V : np.array[num_movies,num_features] -- a matrix of features for each movie\n",
    "\n",
    "        return: float -- the mean squared error between non-zero entries of X and the ratings\n",
    "            predicted by U and V; as this is an error and not a loss function, you do not need to include the\n",
    "            regularizing terms.\n",
    "        \"\"\"\n",
    "    \n",
    "    A = U @ V.T\n",
    "    m = U.shape[0]\n",
    "    n = V.shape[0]\n",
    "    count = 0\n",
    "    error = 0\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if(X[i,j]!=0):\n",
    "                error += (A[i,j]-X[i,j]) * (A[i,j]-X[i,j])\n",
    "                count += 1\n",
    "                \n",
    "    return error/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the training function.\n",
    "\n",
    "### Specification\n",
    "\n",
    "* There is a verbose parameter here; if true, you should evaluate and print the training and test error every few steps. These should decrease (and converge).\n",
    "* You may find it useful to have an indicator matrix W where $W_{ij} = 1$ if there is a rating in $X_{ij}$. \n",
    "* You can initialize U, V with values distributed [normally](numpy.random.normal).\n",
    "* Assume inputs are dense.\n",
    "* The time limits assume a modern laptop; you can ignore them as long as they pass on the server. (Error 143 \n",
    "\n",
    "Hints:\n",
    "\n",
    "* You should be iterating over each row of V and updating it while holding U constant, and then iterating over each row of U while holding V constant.\n",
    "* You should use la.solve (scipy.linalg.solve) to update each row.\n",
    "* The most challenging part of this assignment is writing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Time    | Iter  | Train Err | Test Err |\n",
      "| ------- | ----- | --------- | -------- |\n",
      "|   0.917 |     1 |   13.2762 |  13.2379 |\n",
      "|   6.445 |     2 |    1.7887 |   2.2300 |\n",
      "|  11.807 |     3 |    0.9512 |   1.4484 |\n",
      "|  17.050 |     4 |    0.9216 |   1.4241 |\n",
      "|  22.811 |     5 |    0.9112 |   1.4170 |\n",
      "|  28.051 |     6 |    0.9043 |   1.4126 |\n",
      "|  33.398 |     7 |    0.8990 |   1.4092 |\n",
      "|  40.090 |     8 |    0.8947 |   1.4066 |\n",
      "|  45.966 |     9 |    0.8913 |   1.4046 |\n",
      "|  51.819 |    10 |    0.8885 |   1.4030 |\n",
      "|  56.943 |    11 |    0.8862 |   1.4017 |\n",
      "|  62.241 |    12 |    0.8844 |   1.4006 |\n",
      "\n",
      "| Time    | Iter  | Train Err | Test Err |\n",
      "| ------- | ----- | --------- | -------- |\n",
      "|   1.299 |     1 |   12.9417 |  12.9507 |\n",
      "|   7.070 |     2 |    1.4594 |   2.1017 |\n",
      "|  12.996 |     3 |    0.9518 |   1.6283 |\n",
      "|  18.863 |     4 |    0.8485 |   1.5344 |\n",
      "|  24.895 |     5 |    0.8043 |   1.4899 |\n",
      "|  30.710 |     6 |    0.7797 |   1.4630 |\n",
      "|  36.629 |     7 |    0.7639 |   1.4455 |\n",
      "|  42.460 |     8 |    0.7528 |   1.4335 |\n",
      "|  48.296 |     9 |    0.7448 |   1.4249 |\n",
      "|  55.116 |    10 |    0.7390 |   1.4185 |\n",
      "|  61.532 |    11 |    0.7348 |   1.4136 |\n",
      "|  67.842 |    12 |    0.7316 |   1.4097 |\n",
      "\n",
      "### TESTING train: PASSED 3/6\n",
      "# 0\t: Assertion failed\n",
      "# 2\t: Assertion failed\n",
      "# 5\t: Assertion failed\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_test(train):\n",
    "    ratings, movies_original = read_csv()\n",
    "    X_tr, X_te, movies = process(*ratings, movies_original)\n",
    "    X_tr = X_tr.toarray() # The matrix is small enough we can use the dense form.\n",
    "    X_te = X_te.toarray()\n",
    "    \n",
    "    # Test with k=1\n",
    "    start_time = time.perf_counter()\n",
    "    U, V = train(X_tr, X_te, 1, niters=12)\n",
    "    test.true(time.perf_counter() - start_time < 60)\n",
    "    test.true(error(X_tr, U, V) < 0.9)\n",
    "    test.true(error(X_te, U, V) < 1.4)\n",
    "\n",
    "    # Test with k=3\n",
    "    start_time = time.perf_counter()\n",
    "    U, V = train(X_tr, X_te, 3, niters=12)\n",
    "    test.true(time.perf_counter() - start_time < 120)\n",
    "    test.true(error(X_tr, U, V) < 0.8)\n",
    "    test.true(error(X_te, U, V) < 1.4)\n",
    "    \n",
    "    \n",
    "@test\n",
    "def train(X_train, X_test, k, niters=12, lam=10., verbose=True):\n",
    "    \"\"\" Train a collaborative filtering model. \n",
    "        Args: \n",
    "            X_train : np.array[num_users, num_movies] -- the training ratings matrix, assumed dense\n",
    "            X_test : np.array[num_users, num_movies] -- the test ratings matrix, assumed dense\n",
    "            k : int -- the number of features in the CF model\n",
    "            niters : int -- number of iterations to run\n",
    "            lam : float -- regularization parameter, shown as lambda\n",
    "            verbose : boolean -- if true, print the error on train and test sets every few iterations \n",
    "\n",
    "        return : Tuple[U, V]\n",
    "            U : np.array[num_users,  num_features] -- the user-feature matrix\n",
    "            V : np.array[num_movies, num_features] -- the movie-feature matrix\n",
    "    \"\"\"\n",
    "\n",
    "#     print(X_train.shape)\n",
    "#     print(k)\n",
    "    m,n = X_train.shape\n",
    "    W_train = np.zeros((m,n))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if X_train[i,j] != 0:\n",
    "                W_train[i,j] = 1\n",
    "\n",
    "    U = np.random.normal(size = (m, k))*5\n",
    "    V = np.random.normal(size = (n, k))*5\n",
    "\n",
    "    if verbose:\n",
    "        print(\"| Time    | Iter  | Train Err | Test Err |\")\n",
    "        print(\"| ------- | ----- | --------- | -------- |\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    for i in range(niters):\n",
    "        for u in range(m):\n",
    "            V_ = V * W_train[u,:][:,None]\n",
    "            a = (np.dot(V_.T, V_))\n",
    "            b = (np.dot(X_train[u,:], V))\n",
    "            lamda = np.eye(k) * lam\n",
    "            U[u,:] = la.solve(a + lamda, b)\n",
    "        \n",
    "        for v in range(n):\n",
    "            U_ = U * W_train[:,v][:,None]\n",
    "            a = (np.dot(U_.T, U_))\n",
    "            b = (np.dot(X_train[:,v], U))\n",
    "            lamda = np.eye(k) * lam\n",
    "            V[v,:] = la.solve(a + lamda, b)\n",
    "        \n",
    "        if verbose: \n",
    "            print(f\"| {time.perf_counter() - start_time: 7.3f} |{i+1: 6d} |{error(X_train, U, V):10.4f} |{error(X_test, U, V):9.4f} |\")\n",
    "    \n",
    "    if verbose: \n",
    "        print(\"\")\n",
    "    return U, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to train for 12 iterations, which should be quick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "\n",
    "Finally, we need to be able to make recommendations given a matrix factorization. We can do this by simply taking the recommending the movie with the highest value in the estimated ratings matrix. \n",
    "\n",
    "### Specification\n",
    "* For each user, recommend the the movie with the highest predicted rating for that user that the user **hasn't** seen before. \n",
    "* Return the result as a list of movie Ids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Time    | Iter  | Train Err | Test Err |\n",
      "| ------- | ----- | --------- | -------- |\n",
      "|   1.340 |     1 |   13.0730 |  13.0800 |\n",
      "|   7.067 |     2 |    1.2560 |   1.8544 |\n",
      "|  12.605 |     3 |    0.8634 |   1.4907 |\n",
      "|  18.208 |     4 |    0.7997 |   1.4498 |\n",
      "\n",
      "[('Shawshank Redemption, The (1994)', 253), (\"Schindler's List (1993)\", 47), ('Forrest Gump (1994)', 31), ('Usual Suspects, The (1995)', 28), ('Pulp Fiction (1994)', 26)]\n",
      "### TESTING recommend: PASSED 2/2\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def recommend_test(recommend):\n",
    "    U = np.array([[1., 1., 0., 0.], [0., 1., 0., 1.]])\n",
    "    V = np.array([[.5, 0., 0., .5], [0., .5, .5, 0.], [.5, -1., .5, 3.]])\n",
    "    X = np.array([[5., 0., 0.], [1., 1., 0.]])\n",
    "    recommendation = recommend(X, U, V)\n",
    "    test.true(isinstance(recommendation, list))\n",
    "    test.equal(recommendation, [1, 2])\n",
    "    \n",
    "    # Print the five most commonly recommended movies.\n",
    "    # This part is not graded.\n",
    "    ratings, movies_original = read_csv()\n",
    "    X_tr, X_te, movies = process(*ratings, movies_original)\n",
    "    X_tr = X_tr.toarray()\n",
    "    X_te = X_te.toarray()\n",
    "    U, V = train(X_tr, X_te, 3, niters=4)\n",
    "    recommendation = recommend(X_tr + X_te, U, V)\n",
    "    counts = Counter(recommendation)\n",
    "    print([(movies[i], c) for i, c in counts.most_common(5)])\n",
    "\n",
    "@test\n",
    "def recommend(X, U, V):\n",
    "    \"\"\"Recommend a new movie for every user.\n",
    "\n",
    "        args: \n",
    "            X : np.array[num_users, num_movies] -- the ratings matrix\n",
    "            U : np.array[num_users, num_features] -- a matrix of features for each user\n",
    "            V : np.array[num_movies,num_features] -- a matrix of features for each movie\n",
    "\n",
    "        return: List[int] -- a list of movie Ids for each user\n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    W = np.zeros((m,n))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if X[i,j] == 0:\n",
    "                W[i,j] = 1\n",
    "                \n",
    "    score = W * (U @ V.T)\n",
    "\n",
    "    return [i.tolist().index(max(i)) for i in score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation tends to recommend popular movies, which yours should as well. The exact results vary based on the random spliting of training and test sets.\n",
    "\n",
    "```\n",
    "('Shawshank Redemption, The (1994)', 236),\n",
    "(\"Schindler's List (1993)\", 87),\n",
    "('Streetcar Named Desire, A (1951)', 47),\n",
    "('Usual Suspects, The (1995)', 41),\n",
    "('Forrest Gump (1994)', 37)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
