{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Naive Bayes is a class of simple classifiers based on Bayes' Rule and strong (or naive) independence assumptions between features. In this problem, you will implement a Naive Bayes Classifier for the Census Income Data Set from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/).\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The dataset consists 32561 instances, each representing an individual. The goal is to predict whether a person makes over 50K a year based on 14 features. The features are:\n",
    "\n",
    "| column | type | description |\n",
    "| --- |:---:|:--- |\n",
    "| age | continuous | trips around the sun to date\n",
    "| final_weight | continuous | census weight attribute; constructed from the original census data |\n",
    "| education_num | continuous | numeric education scale -- their maximum educational level as a number |\n",
    "| capital_gain | continuous | income from investment sources |\n",
    "| capital_loss | continuous | losses from investment sources |\n",
    "| hours_per_week | continuous | number of hours worked every week |\n",
    "| work_class | categorical | `Private`, `Self-emp-not-inc`, `Self-emp-inc`, `Federal-gov`, `Local-gov`, `State-gov`, `Without-pay`, `Never-worked` |\n",
    "| education | categorical | `Bachelors`, `Some-college`, `11th`, `HS-grad`, `Prof-school`, `Assoc-acdm`, `Assoc-voc`, `9th`, `7th-8th`, `12th`, `Masters`, `1st-4th`, `10th`, `Doctorate`, `5th-6th`, `Preschool` |\n",
    "| marital_status | categorical | `Married-civ-spouse`, `Divorced`, `Never-married`, `Separated`, `Widowed`, `Married-spouse-absent`, `Married-AF-spouse` |\n",
    "| occupation | categorical | `Tech-support`, `Craft-repair`, `Other-service`, `Sales`, `Exec-managerial`, `Prof-specialty`, `Handlers-cleaners`, `Machine-op-inspct`, `Adm-clerical`, `Farming-fishing`, `Transport-moving`, `Priv-house-serv`, `Protective-serv`, `Armed-Forces` |\n",
    "| relationship | categorical | `Wife`, `Own-child`, `Husband`, `Not-in-family`, `Other-relative`, `Unmarried.` |\n",
    "| race | categorical | `White`, `Asian-Pac-Islander`, `Amer-Indian-Eskimo`, `Other`, `Black` |\n",
    "| sex | categorical | `Female`, `Male` |\n",
    "| native_country | categorical | (41 values not shown here) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "import gzip\n",
    "from testing.testing import test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Data Preparation\n",
    "\n",
    "First, you need to load in the above data, provided to you as a CSV file. As the data is from UCI repository, it is already quite clean. However, some instances contain missing `occupation`, `native_country` or `work_class` (represented as ? in the CSV file) and these have to be discarded from the training set. Also, replace the `income` column with `label`, which is 1 if `income` is `>50K` and 0 otherwise. Finally, ensure you reset the index so the row numbers are contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING load_data: PASSED 6/6\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_csv(fn):\n",
    "    with gzip.open(fn, \"rt\", newline='', encoding=\"UTF-8\") as file:\n",
    "        return pd.read_csv(file)\n",
    "\n",
    "def load_data_test(load_data):\n",
    "    df = load_data()\n",
    "    \n",
    "    DF_TYPES = {\n",
    "        \"age\"  : \"int64\",\n",
    "        \"work_class\"  : \"object\",\n",
    "        \"final_weight\"  : \"int64\",\n",
    "        \"education\"  : \"object\",\n",
    "        \"education_num\"  : \"int64\",\n",
    "        \"marital_status\"  : \"object\",\n",
    "        \"occupation\"  : \"object\",\n",
    "        \"relationship\"  : \"object\",\n",
    "        \"race\"  : \"object\",\n",
    "        \"sex\"  : \"object\",\n",
    "        \"capital_gain\"  : \"int64\",\n",
    "        \"capital_loss\"  : \"int64\",\n",
    "        \"hours_per_week\"  : \"int64\",\n",
    "        \"native_country\"  : \"object\",\n",
    "        \"label\"  : \"int64\"\n",
    "    }\n",
    "\n",
    "    test.equal(DF_TYPES, { k: str(df[k].dtypes) for k in DF_TYPES })\n",
    "\n",
    "    # Check for blank entries:\n",
    "    test.equal(any(df['occupation'].eq(\"?\")), False)\n",
    "    test.equal(any(df['native_country'].eq(\"?\")), False)\n",
    "    test.equal(any(df['work_class'].eq(\"?\")), False)\n",
    "    \n",
    "    # Make sure there's no income column:\n",
    "    test.true(\"income\" not in df.columns)\n",
    "\n",
    "    # Index handling:\n",
    "    test.equal(repr(df.index), \"RangeIndex(start=0, stop=30162, step=1)\")\n",
    "    \n",
    "@test\n",
    "def load_data(file_name=\"census.csv.gz\"):\n",
    "    \"\"\" loads and processes data in the manner specified above\n",
    "\n",
    "\n",
    "    args:\n",
    "        file_name : str -- path to csv file containing data\n",
    "\n",
    "    returns: pd.DataFrame -- processed dataframe\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    df['label'] = 0\n",
    "    dropped = []\n",
    "    for i, row in df.iterrows():\n",
    "        if(row['occupation'] == '?' or row['native_country'] == '?' or row['work_class'] == '?'):\n",
    "            dropped.append(i)\n",
    "        elif(row['income'] == '>50K'):\n",
    "            df.loc[i,'label'] = 1\n",
    "#             df['label'][i] = 1\n",
    "\n",
    "    df = df.drop(index = dropped)    \n",
    "    del df['income']\n",
    "    df = df.reset_index(drop=True)\n",
    "#     print(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Naive Bayes classifier\n",
    "\n",
    "Let $X_1, X_2, \\ldots, X_k$ be the $k$ features of a dataset, with class label given by the variable $y$. A probabilistic classifier assigns the most probable class to each instance $(x_1,\\ldots,x_k)$, as expressed by\n",
    "$$ \\hat{y} = \\arg\\max_y P(y\\ \\mid\\ x_1,\\ldots,x_k) $$\n",
    "\n",
    "Using Bayes' theorem, the above *posterior probability* can be rewritten as\n",
    "$$ P(y\\ \\mid\\ x_1,\\ldots,x_k) = \\frac{P(y) P(x_1,\\ldots,x_n\\ \\mid\\ y)}{P(x_1,\\ldots,x_k)} $$\n",
    "where\n",
    "- $P(y)$ is the prior probability of the class\n",
    "- $P(x_1,\\ldots,x_k\\ \\mid\\ y)$ is the likelihood of data under a class\n",
    "- $P(x_1,\\ldots,x_k)$ is the evidence for data\n",
    "\n",
    "Naive Bayes classifiers assume that the feature values are conditionally independent given the class label, that is,\n",
    "$ P(x_1,\\ldots,x_n\\ \\mid\\ y) = \\prod_{i=1}^{k}P(x_i\\ \\mid\\ y) $. This strong assumption helps simplify the expression for posterior probability to\n",
    "$$ P(y\\ \\mid\\ x_1,\\ldots,x_k) = \\frac{P(y) \\prod_{i=1}^{k}P(x_i\\ \\mid\\ y)}{P(x_1,\\ldots,x_k)} $$\n",
    "\n",
    "For a given input $(x_1,\\ldots,x_k)$, $P(x_1,\\ldots,x_k)$ is constant. Hence, we can say that:\n",
    "$$ P(y\\ \\mid\\ x_1,\\ldots,x_k) \\propto P(y) \\prod_{i=1}^{k}P(x_i\\ \\mid\\ y) $$\n",
    "\n",
    "Thus, the class of a new instance can be predicted as:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{k}P(x_i\\ \\mid\\ y)$$\n",
    "\n",
    "where $P(y)$ is commonly known as the **class prior** and $P(x_i\\ \\mid\\ y)$ is the **feature predictor**.\n",
    "\n",
    "Observe that this is the product of $k+1$ probability values, which can result in very small numbers. When working with real-world data, this often leads to an [arithmetic underflow](https://en.wikipedia.org/wiki/Arithmetic_underflow). We will instead be adding the logarithm of the probabilities:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_y \\underbrace{\\log P(y)}_\\text{log-prior} + \\underbrace{\\sum_{i=1}^{k} \\log P(x_i\\ \\mid\\ y)}_\\text{log-likelihood}$$\n",
    "\n",
    "The rest of the assignment deals with how each of these probability distributions -- $P(y), P(x_1\\ \\mid\\ y), \\ldots, P(x_k\\ \\mid\\ y)$ -- are estimated from data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Predictor\n",
    "\n",
    "Naive Bayes classifiers are popular because we can independently model each feature and mix-and-match model types based on the prior knowledge. For example, we might know (or assume) that $(X_i|y)$ has some distribution, so we can directly use the probability density or mass function of the distribution to model $(X_i|y)$.\n",
    "\n",
    "In this assignment, you will be using two classes of likelihood models:\n",
    "- Gaussian models, for continuous real-valued features (parameterized by mean $\\mu$ and variance $\\sigma$)\n",
    "- Categorical models, for features in discrete categories (parameterized by $\\mathbf{p} = <p_0,p_1\\ldots>$, one parameter per category)\n",
    "\n",
    "You need to implement a generic predictor class for each type of model. Your class should have the following methods:\n",
    "\n",
    "- `fit()`: Learn parameters for the likelihood model using an appropriate Maximum Likelihood Estimator.\n",
    "- `partial_log_likelihood()`: Use the previously learnt parameters to compute the probability density or mass of a given feature value, and return the natural logarithm of this value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Gaussian Feature Predictor\n",
    "\n",
    "The Gaussian distribution is characterized by two parameters - mean $\\mu$ and standard deviation $\\sigma$:\n",
    "$$ f_Z(z) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp{(-\\frac{(z-\\mu)^2}{2\\sigma^2})} $$\n",
    "\n",
    "Given $n$ samples $z_1, \\ldots, z_n$ from the above distribution, the MLE for mean and standard deviation are:\n",
    "$$ \\hat{\\mu} = \\frac{1}{n} \\sum_{j=1}^{n} z_j $$\n",
    "\n",
    "$$ \\hat{\\sigma} = \\sqrt{\\frac{1}{n} \\sum_{j=1}^{n} (z_j-\\hat{\\mu})^2} $$\n",
    "\n",
    "`scipy.stats.norm` may be helpful, as may `pandas.DataFrame.var`. If you use the latter, remember to correctly set the `ddof=0`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING gaussian_pred: PASSED 6/6\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def gaussian_pred_test(gaussian_predictor):\n",
    "    g = gaussian_predictor(2)\n",
    "    \n",
    "    np.random.seed(0xDEADBEEF)\n",
    "    rnd = np.random.normal(loc=0.0, scale=1.0, size=(1000,))\n",
    "\n",
    "    data = pd.Series(np.concatenate([rnd, 100-rnd]))\n",
    "    labels = pd.Series(np.array([0]*1000 + [1]*1000))\n",
    "\n",
    "    g.fit(data, labels)\n",
    "\n",
    "    test.equal(str(type(g.mu)), \"<class 'numpy.ndarray'>\")\n",
    "    test.true(np.allclose(g.mu, [0, 100], atol=.1))\n",
    "    test.equal(str(type(g.sigma)), \"<class 'numpy.ndarray'>\")\n",
    "    test.true(np.allclose(g.sigma, [1, 1], atol=.1))\n",
    "\n",
    "    test.equal(tuple(g.partial_log_likelihood([0., 50., 100.]).shape), (2, 3))\n",
    "    # If the equality is not exact, you may need to change the test to ensure the absolute difference is no more than 1e-4\n",
    "    test.true(np.allclose(g.partial_log_likelihood([0., 50., 100.]), [[-0.9234573135702573, -1242.233086628376, -4963.217354198167], [-4963.217354198166, -1242.2330866283753, -0.9234573135702564]], rtol=0, atol=1e-4))\n",
    "\n",
    "class GaussianPredictor:\n",
    "    \"\"\" Feature predictor for a normally distributed real-valued, continuous feature.\n",
    "\n",
    "        attr:\n",
    "            k : int -- number of classes\n",
    "            mu : np.ndarray[k] -- vector containing per class mean of the feature\n",
    "            sigma : np.ndarray[k] -- vector containing per class std. deviation of the feature\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        \"\"\" constructor\n",
    "\n",
    "        args : k -- number of classes\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"update predictor statistics (mu, sigma) for Gaussian distribution\n",
    "\n",
    "        args:\n",
    "            x : pd.Series -- feature values\n",
    "            y : np.Series -- class labels\n",
    "            \n",
    "        return : GaussianPredictor -- return self for convenience\n",
    "        \"\"\"\n",
    "        n = len(x)\n",
    "        classes = list(set(y))\n",
    "        count = np.zeros(self.k)\n",
    "\n",
    "        mu = np.zeros(self.k)\n",
    "        sigma = np.zeros(self.k)\n",
    "        \n",
    "        for i in range(n):\n",
    "            mu[classes.index(y[i])] += x[i]\n",
    "            count[classes.index(y[i])] += 1\n",
    "        self.mu = mu/count\n",
    "        \n",
    "        for i in range(n):\n",
    "            sigma[classes.index(y[i])] += (x[i] - self.mu[classes.index(y[i])])**2\n",
    "        self.sigma = np.sqrt(sigma/count)\n",
    "#         print(self.mu)\n",
    "#         print(self.sigma)\n",
    "        return self\n",
    "            \n",
    "    def partial_log_likelihood(self, x):\n",
    "        \"\"\" log likelihood of feature values x according to each class\n",
    "\n",
    "        args:\n",
    "            x : pd.Series -- feature values\n",
    "\n",
    "        return: np.ndarray[self.k, len(x)] : log likelihood for this feature for each class\n",
    "        \"\"\"\n",
    "\n",
    "        result = np.zeros((self.k, len(x)))\n",
    "        for i,value in enumerate(x):\n",
    "            result[:, i] = stats.norm.logpdf(np.ones(self.k) * value, loc = self.mu, scale = self.sigma)\n",
    "        return result\n",
    "\n",
    "@test\n",
    "def gaussian_pred(k):\n",
    "    return GaussianPredictor(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Categorical Feature Predictor\n",
    "\n",
    "The categorical distribution with $l$ possible values $\\{\\text A, \\text B, \\text C, \\ldots\\}$ is characterized by the probability distribution $\\mathbf p$ over these values. ($\\mathbf{p} = (p_0,\\dots,p_{l-1})$ where $\\sum\\mathbf p = 1$.)\n",
    "\n",
    "If $C$ is categorically distributed, the probability of observing a particular value $z$ is:\n",
    "\n",
    "$$ \\Pr(C=z; \\mathbf{p}) = \\begin{cases}\n",
    "    p_0 & \\text{ if } z=0\n",
    "\\\\  p_1 & \\text{ if } z=1\n",
    "\\\\  \\vdots\n",
    "\\\\  p_{l-1} & \\text{ if } z=(l-1)\n",
    "\\end{cases}$$\n",
    "\n",
    "Given $n$ samples $z_1, \\ldots, z_n$ from $C$, the smoothed Maximum Likelihood Estimator for $\\mathbf p$ is:\n",
    "$$ \\hat{p_t} = \\frac{\\sum_{j=1}^{n} [z_j=t] + \\alpha}{n + l\\alpha} $$\n",
    "\n",
    "The term in the numerator $\\sum_{j=1}^{n} [z_j=t]$ is the number of times the value $t$ occurred in the sample. The smoothing is done over all possible values that may be generated by $C$. (This avoids the zero-count problem, similar to the smooting done in $n$-gram language models.)\n",
    "\n",
    "In this problem, you need to write a predictor that learns a different categorical distribution $C_i$ for each of $k$ possible classes.\n",
    "\n",
    "#### Specification\n",
    "\n",
    "  1. You should maintain a dictionary from each possible input token (i.e. each value) to an array of length $k$ that contains $(\\Pr(C_0=z), \\Pr(C_1=z), ..., \\Pr(C_{k-1}=z))$.\n",
    "  2. For the purpose of smoothing, you should assume that all distributions can produce each value. That is, the set of possible values is the same for all $C_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING categorical_pred: PASSED 8/8\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def categorical_pred_test(categorical_pred):\n",
    "    # Test One:\n",
    "    p = categorical_pred(3)\n",
    "    \n",
    "    data = pd.Series([\"A\"]*99 + [\"B\"]*99 + [\"C\"]*99)\n",
    "    labels = pd.Series([0]*99 + [1]*99 + [2]*99)\n",
    "    p.fit(data, labels)\n",
    "    \n",
    "    test.true(np.allclose(p.p['A'], [0.98039216, 0.00980392, 0.00980392], atol=1e-6))\n",
    "\n",
    "    pll = p.partial_log_likelihood([\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"])\n",
    "    test.equal(tuple(pll.shape), (3, 6))\n",
    "    n = np.log(1/102)\n",
    "    p = np.log(100/102)\n",
    "    \n",
    "    test.true(np.allclose(pll, [[p, n, n, p, n, n], [n, p, n, n, p, n], [n, n, p, n, n, p]]))\n",
    "    \n",
    "    # Test Two:\n",
    "    \n",
    "    p = categorical_pred(2)\n",
    "    \n",
    "    data = pd.Series([\"A\"]*50 + [\"B\"]*50 + [\"C\"]*50)\n",
    "    labels = pd.Series([0]*75 + [1]*75)\n",
    "    p.fit(data, labels)\n",
    "    \n",
    "    test.true(np.allclose(p.p['A'], [0.65384614, 0.01282051], atol=1e-6))\n",
    "    test.true(np.allclose(p.p['B'], [0.33333334, 0.33333334], atol=1e-6))\n",
    "    test.true(np.allclose(p.p['C'], [0.01282051, 0.65384614], atol=1e-6))\n",
    "\n",
    "    pll = p.partial_log_likelihood([\"A\", \"B\", \"C\"])\n",
    "    test.equal(tuple(pll.shape), (2, 3))\n",
    "    n = np.log(1/78)\n",
    "    m = np.log(51/78)\n",
    "    l = np.log(26/78)\n",
    "\n",
    "    test.true(np.allclose(pll, [[m, l, n], [n, l, m]], atol=1e-6))\n",
    "#     If test two fails but test one passes, check the smoothing term in the denominator!\n",
    "\n",
    "class CategoricalPredictor:\n",
    "    \"\"\" Feature predictor for a categorical feature.\n",
    "\n",
    "        attr: \n",
    "            k : int -- number of classes\n",
    "            p : Dict[feature_value, np.ndarray[k]] -- dictionary of vectors containing per-class probability of a feature value;\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        \"\"\" constructor\n",
    "\n",
    "        args : k -- number of classes\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, x, y, alpha=1.):\n",
    "        \"\"\" initializes the predictor statistics (p) for Categorical distribution\n",
    "        \n",
    "        args:\n",
    "            x : pd.Series -- feature values\n",
    "            y : pd.Series -- class labels\n",
    "        \n",
    "        kwargs:\n",
    "            alpha : float -- smoothing factor\n",
    "\n",
    "        return : CategoricalPredictor -- returns self for convenience\n",
    "        \"\"\"\n",
    "        n = len(x)\n",
    "        c_x = list(set(x))\n",
    "        c_y = list(set(y))\n",
    "\n",
    "        count = np.zeros(self.k)\n",
    "        p = np.zeros((len(c_x),self.k))\n",
    "        \n",
    "        for i in range(n):\n",
    "            p[c_x.index(x[i]),c_y.index(y[i])] += 1\n",
    "            count[c_y.index(y[i])] += 1\n",
    "        p = (p + alpha)/(count + len(c_x)*alpha)\n",
    "        \n",
    "        result = {}\n",
    "        for i,prob in enumerate(p):\n",
    "            result[c_x[i]] = prob\n",
    "        self.p = result\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def partial_log_likelihood(self, x):\n",
    "        \"\"\" log likelihood of feature values x according to each class\n",
    "\n",
    "        args:\n",
    "            x : pd.Series -- vector of feature values\n",
    "\n",
    "        return : np.ndarray[self.k, len(x)] -- matrix of log likelihood for this feature\n",
    "        \"\"\"\n",
    "        result = np.zeros((self.k, len(x)))\n",
    "        for i,value in enumerate(x):\n",
    "            result[:,i] = np.log(self.p[value])\n",
    "        return result\n",
    "\n",
    "@test\n",
    "def categorical_pred(k):\n",
    "    return CategoricalPredictor(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Putting things together\n",
    "\n",
    "It's time to put all the feature predictors together and do something useful! You will implement a class that puts these classifiers to good use:\n",
    "\n",
    "- `__init__()`: Compute the log prior for each class and initialize the feature predictors (based on feature type). The smoothed prior for class $t$ is given by\n",
    "$$ \\text{prior}(t) = \\frac{n_t + \\alpha}{n + k\\alpha} $$\n",
    "where $n_t = \\sum_{j=1}^{n} [y_j=t]$, (i.e., the number of times the label $t$ occurred in the sample), $n$ is the number fo entries in the sample, and $k$ is the number of label values. \n",
    "- `log_likelihood()`: Compute the sum of the log prior and partial log likelihoods for all features. Use it to predict the final class label.\n",
    "- `predict()`: Use the output of log_likelihood to predict a class label; break ties by predicting the class with lower id.\n",
    "\n",
    "**Note:** Your implementation should not assume the data will always be the same as the census data. We may pass any dataset to your class. You can assume that:\n",
    "\n",
    "1. the input will contain a `label` column of type `int64` with values $0,\\ldots,k-1$ for some $k$\n",
    "2. all other columns will be either of type `object` (for categorical data) or `int64` (for integer data)\n",
    "3. if you encounter a column of an invalid type, throw an exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING naive_bayes: PASSED 19/20\n",
      "# 16\t: Failed: [[-49.84977999441482, -50.38520793710976], [-53.40738377703323, -51.30832341372785]] is not equal to [[-49.84977999441486, -50.38520793711001], [-53.407383777033196, -51.30832341372758]]\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def naive_bayes_test(naive_bayes):\n",
    "    df = load_data()\n",
    "    cl = naive_bayes(df)\n",
    "    test.equal(cl.log_prior.tolist(), [-0.28626858222129903, -1.3905468592226538])\n",
    "\n",
    "    test.true(isinstance(cl.predictor['age'], GaussianPredictor))\n",
    "    test.true(isinstance(cl.predictor['work_class'], CategoricalPredictor))\n",
    "    test.true(isinstance(cl.predictor['final_weight'], GaussianPredictor))\n",
    "    test.true(isinstance(cl.predictor['education'], CategoricalPredictor))\n",
    "    test.true(isinstance(cl.predictor['education_num'], GaussianPredictor))\n",
    "    test.true(isinstance(cl.predictor['marital_status'], CategoricalPredictor))\n",
    "    test.true(isinstance(cl.predictor['occupation'], CategoricalPredictor))\n",
    "    test.true(isinstance(cl.predictor['relationship'], CategoricalPredictor))\n",
    "    test.true(isinstance(cl.predictor['race'], CategoricalPredictor))\n",
    "    test.true(isinstance(cl.predictor['sex'], CategoricalPredictor))\n",
    "    test.true(isinstance(cl.predictor['capital_gain'], GaussianPredictor))\n",
    "    test.true(isinstance(cl.predictor['capital_loss'], GaussianPredictor))\n",
    "    test.true(isinstance(cl.predictor['hours_per_week'], GaussianPredictor))\n",
    "    test.true(isinstance(cl.predictor['native_country'], CategoricalPredictor))    \n",
    "\n",
    "    ll = cl.log_likelihood(df.drop(\"label\", axis=\"columns\"))\n",
    "    test.equal(tuple(ll.shape), (2, 30162))\n",
    "    test.equal(ll[:,:2].tolist(), [[-49.84977999441486, -50.38520793711001], [-53.407383777033196, -51.30832341372758]])\n",
    "\n",
    "    lp = cl.predict(df.drop(\"label\", axis=\"columns\"))\n",
    "    test.equal(tuple(lp.shape), (30162,))\n",
    "    test.equal(sum(lp), 5407)\n",
    "    test.equal(lp[:10].tolist(), [0]*8 + [1]*2)\n",
    "    \n",
    "#     # Make sure you can handle a non-contiguous index, pt. 1:\n",
    "#     cl = naive_bayes(df[1::3])\n",
    "#     ll = cl.log_likelihood(df.drop(\"label\", axis=\"columns\"))\n",
    "#     test.equal(tuple(ll.shape), (2, 30162))\n",
    "\n",
    "\n",
    "#     # Make sure you can handle a non-contiguous index, pt. 2:\n",
    "#     tdf = df.drop([\"age\", \"sex\", \"work_class\"], axis=\"columns\")\n",
    "#     cl = naive_bayes(tdf[1::2])\n",
    "#     ll = cl.log_likelihood(tdf.drop(\"label\", axis=\"columns\"))\n",
    "#     test.equal(tuple(ll.shape), (2, 30162))\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    \"\"\" Naive Bayes classifier for a mixture of continuous and categorical attributes.\n",
    "        We use GaussianPredictor for continuous attributes and CategoricalPredictor for categorical ones.\n",
    "        \n",
    "        attr:\n",
    "            predictor : Dict[column_name,model] -- model for each column\n",
    "            log_prior : np.ndarray -- the (log) prior probability of each class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, alpha=1.):\n",
    "        \"\"\"initialize predictors for each feature and compute class prior\n",
    "        \n",
    "        args:\n",
    "            df : pd.DataFrame -- processed dataframe, without any missing values.\n",
    "        \n",
    "        kwargs:\n",
    "            alpha : float -- smoothing factor for prior probability\n",
    "        \"\"\"\n",
    "        predictor = {}\n",
    "        k = len(set(df['label']))\n",
    "        self.k = k\n",
    "        for title in df.drop(\"label\", axis=\"columns\"):\n",
    "#             print(df[title].dtype)\n",
    "            if (df[title].dtype == \"int64\"):\n",
    "                predictor[title] = GaussianPredictor(k)\n",
    "                predictor[title].fit(list(df[title]),list(df['label']))\n",
    "            elif (df[title].dtype == \"object\"):\n",
    "                predictor[title] = CategoricalPredictor(k)\n",
    "                predictor[title].fit(list(df[title]),list(df['label']))\n",
    "#             raise ValueError(df[title].dtype)\n",
    "#         print(predictor)\n",
    "        self.predictor = predictor\n",
    "    \n",
    "        count = np.zeros(k)\n",
    "        classes = list(set(df['label']))\n",
    "        df = df.reset_index(drop=True)\n",
    "        n = len(df['label'])\n",
    "        for i in range(n):\n",
    "#             print(df['label'])\n",
    "#             print(classes.index(df['label'][i]))\n",
    "            count[classes.index(df['label'][i])] += 1\n",
    "        self.log_prior = np.log((count + alpha)/(n + k * alpha))\n",
    "\n",
    "    def log_likelihood(self, x):\n",
    "        \"\"\"log_likelihood for input instances from log_prior and partial_log_likelihood of feature predictors\n",
    "\n",
    "        args:\n",
    "            x : pd.DataFrame -- processed dataframe (ignore label if present)\n",
    "\n",
    "        returns : np.ndarray[num_classes, len(x)] -- array of log-likelihood\n",
    "        \"\"\"\n",
    "        \n",
    "        ll = np.zeros((self.k, len(x)))\n",
    "#         print(len(x))\n",
    "        for i in range(len(x)):\n",
    "            ll[:,i] = self.log_prior\n",
    "#         for title in x:\n",
    "#             print(title)\n",
    "        for title in x:\n",
    "#             print(title)\n",
    "#             print(self.predictor[title])\n",
    "            if(title != \"label\"):\n",
    "                ll += self.predictor[title].partial_log_likelihood(list(x[title]))\n",
    "#         print(self.predictor['age'].partial_log_likelihood(list(df['age'])))\n",
    "        return ll\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"predicts label for input instances, breaks ties in favor of the class with lower id.\n",
    "\n",
    "        args:\n",
    "            x : pd.DataFrame -- processed dataframe (ignore label if present)\n",
    "\n",
    "        returns : np.ndarray[len(x)] -- vector of class labels\n",
    "        \"\"\"\n",
    "        prediction = np.zeros(len(x))\n",
    "        ll = self.log_likelihood(x)\n",
    "        labels = ll.argmax(axis = 0)\n",
    "        return labels\n",
    "\n",
    "@test\n",
    "def naive_bayes(*args, **kwargs):\n",
    "    return NaiveBayesClassifier(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# k = len(set(df['label']))\n",
    "# predictor = {}\n",
    "# for title in df:\n",
    "# #     print(title)\n",
    "#     if (df[title].dtype == \"int64\"):\n",
    "#         predictor[title] = GaussianPredictor(k)\n",
    "#         predictor[title].fit(df[title],df['label'])\n",
    "#     elif (df[title].dtype == \"object\"):\n",
    "#         predictor[title] = CategoricalPredictor(k)\n",
    "#         predictor[title].fit(df[title],df['label'])\n",
    "# # self.predictor = predictor\n",
    "# # print(predictor)\n",
    "# print(len(set(df[\"label\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.28626858 -1.39054686]\n"
     ]
    }
   ],
   "source": [
    "# count = np.zeros(2)\n",
    "# classes = list(set(df['label']))\n",
    "\n",
    "# n = len(df['label'])\n",
    "# for i in range(n):\n",
    "#     count[classes.index(df['label'][i])] += 1\n",
    "# log_prior = np.log((count + 1)/(n + 2 * 1))\n",
    "# print(log_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 30162)\n"
     ]
    }
   ],
   "source": [
    "# x = df.drop(\"label\", axis=\"columns\")\n",
    "# # print(x)\n",
    "# # cl = naive_bayes(df)\n",
    "# a = predictor['age'].partial_log_likelihood(list(df['age']))\n",
    "# print(a.shape)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
