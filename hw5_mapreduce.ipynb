{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce\n",
    "\n",
    "MapReduce is a popular paradigm for distributed computing on big data. While it has been replaced by generalized dataflow systems like [Apache Spark](https://spark.apache.org/) and [Microsoft Dryad](https://www.microsoft.com/en-us/research/project/dryad/), it is a very simple concept that lends itself to distributed computing.\n",
    "\n",
    "Here you will implement a simple version of MapReduce and use it to solve problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING nltk_download: PASSED 0/0\n",
      "###\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iamye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\iamye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "from typing import TypeVar, Hashable, Dict, List, Callable, Iterable, Tuple, Optional\n",
    "import collections\n",
    "import tarfile\n",
    "import gzip\n",
    "import itertools\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from testing.testing import test\n",
    "\n",
    "def nltk_download_test(nltk_download):\n",
    "    nltk_download()\n",
    "\n",
    "@test\n",
    "def nltk_download():\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce Implementation\n",
    "\n",
    "MapReduce is conceptually very simple. Read the lecture notes, and optionally the [Wikipedia Entry](https://en.wikipedia.org/wiki/MapReduce) to understand it. Here's a summary:\n",
    "\n",
    "1. The Map function is called on each element of a list of inputs, producing a list of dictionaries. Each input (in any format) is converted to a dictionary from a key to some intermediate value. Once you figure out what a good intermediate value for the problem you're trying to solve, the Map and Reduce functions are usually obvious. \n",
    "2. The Partition function (sometimes called Shuffle or Collate) converts the produced list-of-dictionaries into a dictionary-of-lists. Each key/value pair in the output dictionay is sometimes called a Partition.\n",
    "3. The Reduce function converts each value in the dictionary-of-lists into a single value, making it a dictionary-of-values.\n",
    "\n",
    "Now you have to complete this implementation of MapReduce by filling in the partition function. We have included type hints, but you don't need them to complete this; you can optionally read more about them [here](https://mypy.readthedocs.io/en/latest/cheat_sheet_py3.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING partition: PASSED 1/1\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Type variables for typechecking:\n",
    "# This is a way to denote the input and output types of mapreduce and various functions.\n",
    "# We also describe this information in the mapreduce docstring below\n",
    "\n",
    "TInput = TypeVar(\"TInput\")             # Type of input\n",
    "TKey = TypeVar(\"TKey\", bound=Hashable) # Key type, must be hashable \n",
    "TIntermediate = TypeVar(\"TIntermediate\") # Intermediate representation\n",
    "TOutput = TypeVar(\"TOutput\")           # The produced output type\n",
    "\n",
    "def partition_test(partition):\n",
    "    test.equal(partition([{\"A\": \"A\", \"B\": \"B\"}, {\"B\": \"B\", \"C\": \"C\"}]),\n",
    "              {\"A\": [\"A\"], \"B\": [\"B\", \"B\"], \"C\": [\"C\"]})\n",
    "\n",
    "@test\n",
    "def partition(intermediates : Iterable[Dict[TKey, TIntermediate]]) -> Dict[TKey, List[TIntermediate]]:\n",
    "        \"\"\"Organize the mapped values by their key.\n",
    "\n",
    "        args:\n",
    "            intermediates : Iterable[Dict[TKey, TIntermediate]] -- a list of dictionaries produced by the mapper\n",
    "\n",
    "        returns : Dict[TKey, List[TIntermediate]] -- the values in intermediates grouped by key \n",
    "        \"\"\"\n",
    "        \n",
    "        rv = collections.defaultdict(list)\n",
    "        for dic in intermediates:\n",
    "#             print(dic)\n",
    "            for word in dic:\n",
    "#                 print(word)\n",
    "                rv[word].append(dic[word])\n",
    "        \n",
    "        return rv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have done that, the MapReduce implementation is quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a fake multiprocessing.Pool, used to provide single-thread execution\n",
    "# of map. You get better error messages with this.\n",
    "class FakePool():\n",
    "    def map(self, lmbd, data, chunksize=None):\n",
    "        return list([lmbd(x) for x in data])\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    def __exit__(self, a1, a2, a3):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapreduce(mapper  : Callable[[TInput], Dict[TKey, TIntermediate]],\n",
    "              reducer : Callable[[Tuple[TKey, List[TIntermediate]]], TOutput],\n",
    "              data    : Iterable[TInput],\n",
    "              chunksize_map=1, chunksize_reduce=8, pool=None) -> Dict[TKey, TOutput]:\n",
    "    \"\"\" MapReduce. Map the data using mapper, partition it, and reduce each partition using the reducer.\n",
    "    \n",
    "    args:\n",
    "        mapper  : Callable[[TInput], Dict[TKey, TIntermediate]]\n",
    "                -- a function that takes in an input element and breaks it out into a dictionary from some key\n",
    "                   to some intermediate value\n",
    "        reducer : Callable[[Tuple[TKey, List[TIntermediate]]], TOutput]\n",
    "                -- a function that takes in a tuple of the key and the list of intermediate values and converts\n",
    "                   it to a single output value; this conversion should not rely on the key\n",
    "        data    : Iterable[TInput] -- the input data to map over\n",
    "\n",
    "    kwargs:\n",
    "        chunksize_map    : int -- an internal parameter used for performance-tuning\n",
    "        chunksize_reduce : int -- an internal parameter used for performance-tuning\n",
    "        pool : multiprocessing.Pool -- the pool providing mapping\n",
    "\n",
    "    returns : List[TOutput] -- the output of running the mapper and reducer on data.\n",
    "    \"\"\"\n",
    "    if pool is None:\n",
    "        pool = FakePool()\n",
    "    \n",
    "    # Isn't the implementation really simple?\n",
    "    intermediates = pool.map(mapper, data, chunksize_map)\n",
    "    partitions = partition(intermediates)\n",
    "    return pool.map(reducer, partitions.items(), chunksize_reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're using the Python multiprocessing library to split the computation across the cores in your computer. There are [some limitations](https://codewithoutrules.com/2018/09/04/python-multiprocessing/) to this: in particular, you cannot use anonymous functions (i.e. `lambda` expressions). If your Python kernel crashes, you may need to reset it.\n",
    "\n",
    "Great! Now let's apply this to a very simple example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Count\n",
    "\n",
    "A classic example of a MapReduce application is in the counting of words in a corpus; here we count _characters_ instead. The map function converts each documentation into the count of words that appear in the document, and the reduce function adds the counts for each word up, producing a tuple as the output type. The keys are words, and the intermediate representation is the count in each string, and the output is a tuple of character and count.\n",
    "\n",
    "Now, go through the code below and make sure you understand how to use MapReduce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cc_map(data : str) -> collections.Counter:\n",
    "    \"\"\" Read a string and convert it into character frequency dictionary\n",
    "\n",
    "    args:\n",
    "        data : str -- the input string\n",
    "    \n",
    "    returns : Counter[str, int] -- a dictionary of counts, from each character to the number of times it appears\n",
    "    \"\"\"\n",
    "    return collections.Counter(data)\n",
    " \n",
    "def cc_reduce(t : Tuple[str, List[int]]):\n",
    "    \"\"\" Find the total number of times a character appears in the input from a list of appearances in each file\n",
    "    \n",
    "    args:\n",
    "        t : Tuple[key, frequencies] -- a tuple of\n",
    "            key : str -- the character\n",
    "            frequencies : List[int] -- the number of times it appears in each file\n",
    "    \n",
    "    returns: Tuple[str, int] -- a tuple of (word, total number of occurrences)\n",
    "    \"\"\"\n",
    "    key, values = t\n",
    "    return (key, sum(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', 2000723), ('e', 1108892), ('t', 800450), ('a', 698889), ('o', 662391), ('h', 635854), ('n', 607945), ('s', 540033)]\n",
      "### TESTING count_characters: PASSED 1/1\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_characters_test(count_characters):\n",
    "    cc = count_characters([nltk.corpus.gutenberg.raw(f) for f in nltk.corpus.gutenberg.fileids()])\n",
    "    cc.sort(key=lambda x: x[1], reverse=True)\n",
    "    test.equal(cc[:8], [(' ', 2000723), ('e', 1108892), ('t', 800450), ('a', 698889), ('o', 662391), ('h', 635854), ('n', 607945), ('s', 540033)])    \n",
    "    # The most common letter is the space character, followed by the letter 'e'.\n",
    "    print(cc[:8])\n",
    "\n",
    "@test\n",
    "def count_characters(alos):\n",
    "    \"\"\" Count the number of times each character appears in the input data\n",
    "    \n",
    "    args:\n",
    "      alos : List[str] -- a list of strings\n",
    "    \"\"\"\n",
    "    return mapreduce(cc_map, cc_reduce, alos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's kick it up a notch with the poster child use-case for MapReduce.\n",
    "\n",
    "## Inverted Index\n",
    "\n",
    "An inverted index is a mapping (`dict`) of elements to the list of locations at which the element occurs. When Google released their [2004 MapReduce paper](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf), this was one of the use-cases they cited.\n",
    "\n",
    "The concept of an inverted index is central to any modern search engine. E.g., given a query \"X, Y\", where, we want to retrieve documents containing the words X or/and Y, the engine only needs to look at the union/intersection of the documents in the inverted indices of X and Y (which have been precomputed already)!\n",
    "\n",
    "In our version of this, we will be constructing an inverted index for words in documents in the Project Gutenberg corpus.\n",
    "\n",
    "### Specification\n",
    "\n",
    "The types in this problem are:\n",
    "\n",
    " - The input type is `Tuple[str, List[str]]`, corresponding to the file name and a list of words\n",
    " - The key type is `str`, corresponding to each word\n",
    " - The intermediate representation type is `List[Tuple[str, int]]`, which is a list of (filename, position) pairs\n",
    " - The output type is `Tuple[str, TIntermediate]`, which is a tuple of word and the intermediate representations, with the added constraint that the intermediate representations are in sorted order.\n",
    "\n",
    "(To make this easier, we've told you what the intermediate representation is -- when writing MapReduce problems, finding the correct intermediate representation is often the challenge.)\n",
    "\n",
    "In the map step, your output should not include any stopwords in `nltk.corpus.stopwords.words('english')`. You do not need to lower-case the input or lemmatize words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ii_map(data : Tuple[str, List[str]]) -> Dict[str, List[Tuple[str, int]]]:\n",
    "    \"\"\" Inverted index map; compile a list of occurrences of each (non-stopword) word in a document.\n",
    "\n",
    "    args:\n",
    "        data : Tuple[filename, words] -- the input data, where:\n",
    "            filename : str -- the filename to tag each word in the index with\n",
    "            words : List[str] -- the list of words in the document, in the order they appear\n",
    "\n",
    "    returns : Dict[str, List[Tuple[str, int]] -- the mapped dictionary, where each word has a corresponding list\n",
    "        of tuples, each with the filename and index at which it appears.\n",
    "    \"\"\"\n",
    "    sw = set(stopwords.words('english'))\n",
    "    filename, words = data\n",
    "    dic = collections.defaultdict(list)\n",
    "    for i, word in enumerate(words):\n",
    "        if(word not in sw):\n",
    "            dic[word].append((filename, i))\n",
    "    \n",
    "    return dic\n",
    "\n",
    "def ii_reduce(t : Tuple[str, List[List[Tuple[str, int]]]]):\n",
    "    \"\"\" Assemble and sort the inverted index.\n",
    "\n",
    "    args:\n",
    "        t : Tuple[key, List[occurrences]] -- a tuple of\n",
    "            key : str -- the word \n",
    "            occurrences : List[Tuple[str, int]] -- the file name and word number in the file\n",
    "\n",
    "    returns : Tuple[key, occurrences] -- a tuple of\n",
    "            key : str -- the word \n",
    "            occurrences : List[Tuple[str, int]] -- the occurrences, flattened and sorted\n",
    "\n",
    "    \"\"\"\n",
    "    key, occurrences = t\n",
    "#     print(key)\n",
    "    new_occurrences = []\n",
    "    for o in occurrences:\n",
    "        for oo in o:\n",
    "            new_occurrences.append(oo)\n",
    "#     print(occurrences)\n",
    "    return(key, sorted(new_occurrences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING inverted_index: PASSED 12/12\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def inverted_index_test(inverted_index):\n",
    "    tt = inverted_index([(\"@\", list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")), (\"%\", list(\"WXYZ\"))])\n",
    "    test.equal(tt[:4], [('A', [('@', 0)]), ('B', [('@', 1)]), ('C', [('@', 2)]), ('D', [('@', 3)])])\n",
    "    test.equal(tt[-2], ('Y', [('%', 2), ('@', 24)]))\n",
    "    test.equal(tt[-1], ('Z', [('%', 3), ('@', 25)]))\n",
    "\n",
    "    books = ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt']\n",
    "    gutenberg = inverted_index([(f.replace(\".txt\", \"\").replace(\"austen-\", \"\"), list(nltk.corpus.gutenberg.words(f))) for f in books])\n",
    "    g = dict(gutenberg)\n",
    "\n",
    "    # Check that you have all entries for some words:\n",
    "    test.equal(len(g[\",\"]), 27601)\n",
    "    test.equal(len(g[\"I\"]), 6306)\n",
    "    test.equal(len(g[\"could\"]), 1837)\n",
    "    test.equal(len(g[\"Mr\"]), 1587)\n",
    "    test.equal(len(g[\"Emma\"]), 866)\n",
    "\n",
    "    # Check output for some words:\n",
    "    test.equal(g[\"Austen\"], [('emma', 4), ('persuasion', 4), ('sense', 6)])\n",
    "    # If this works but the next test fails, then you aren't ordering the \n",
    "    test.equal(set(g[\"likelihood\"]), set([('emma', 5715), ('emma', 110230), ('persuasion', 68360), ('sense', 83037), ('sense', 120041)]))\n",
    "    test.equal(g[\"likelihood\"], [('emma', 5715), ('emma', 110230), ('persuasion', 68360), ('sense', 83037), ('sense', 120041)])\n",
    "    \n",
    "    # Check which files the name \"Emma\" is mentioned in:\n",
    "    test.equal(collections.Counter(v for v, _ in g[\"Emma\"]), {\"emma\": 865, \"persuasion\": 1})\n",
    "\n",
    "@test\n",
    "def inverted_index(data):\n",
    "    \"\"\" Count the number of times each character appears in the input data\n",
    "    \n",
    "    args:\n",
    "      data : Tuple[str, List[str]] -- a list of (filename, words)\n",
    "    \"\"\"\n",
    "    return mapreduce(ii_map, ii_reduce, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Web Graph\n",
    "\n",
    "The world-wide web can be throught of as a directed graph, where the edge A → B encodes a link in page A that points to page B. We've previously covered adjacency matrices in this course; we can very easily construct a row in the adjacency matrix for each page by parsing each webpage and recording the outgoing links we observe in it.\n",
    "\n",
    "Now we want the *reverse* adjacency list. For each webpage, we want the list of webpages which point to it (i.e., incoming links). This is the reverse webgraph problem, and we can solve this with MapReduce:\n",
    "\n",
    "### Specification\n",
    "\n",
    "The types in this problem are:\n",
    "\n",
    " - The input type is `Tuple[str, bytes]`, corresponding to the URL and the raw bytes of the website\n",
    " - The key type is `Tuple[str, str]`, corresponding to the URL `netloc` and `path`. We explain these parts in the Hints section.\n",
    " - The intermediate type is up to you!\n",
    " - The output type is `Tuple[key, Dict[incoming_netloc, incoming_path]]`, where:\n",
    "     - `key` is a `Tuple[str, str]`, same as the key type.\n",
    "     - `incoming_netloc` is a `str` which has the `netloc` of the incoming links, and\n",
    "     - `incoming_path` is a `Set[str]` which is a set of `path`s following the associated `netloc`.\n",
    "\n",
    "Details:\n",
    "\n",
    "- When constructing the index, we do not care how many links point from A to B.\n",
    "- You can choose any way to represent the data.\n",
    "- You should use `BeautifulSoup` to parse the webpage.\n",
    "    - Only search for URLs in tags like `<a href=\"...\">`.\n",
    "    - Only consider URLs ending in `.html`\n",
    "- URLs can be relative or absolute; you should use [`urlparse`](https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urlparse) to extract the `netloc` and `path` to determine which path this is.\n",
    "    - If the URL is absolute (such as `http://cs.cornell.edu/Info/Courses/Current/CS415/CS414.html`), then `netloc` will not be blank. You can use the parsed `netloc` and `path` directly.\n",
    "    - If the URL is relative (such as `/Info/Courses/Current/CS415/CS414.html` or `CS414.html`), then `netloc` will be blank. You can assume the `netloc` of the target is the same as the `netloc` of the source, and:\n",
    "        - If the `path` has a leading `/`, then it is the complete path of the target.\n",
    "        - If the `path` does not have a leading `/`, you should replace the final path segment (i.e. everything after the final `/`) with the contents of `path`. You do not need to consider parent paths (i.e. `../`).\n",
    "            - if the `path` is empty, then you should replace the entire path with the target `path`, adding a leading `/`.\n",
    "- Not all target pages may appear in the source; this is alright.\n",
    "- Ignore the `Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.` message.\n",
    "\n",
    "The most fiddly part of this problem is in handling URLs; we provide you with the stub `canonicalize_url` and some tests which you can optionally use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING canonicalize_url: PASSED 11/11\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def canonicalize_url_test(canonicalize_url):\n",
    "    source = (\"source.edu\", \"/path/segment/leaf.html\")\n",
    "    test.equal(canonicalize_url(source, \"https://a.edu/research/test.html\"), (\"a.edu\", \"/research/test.html\"))\n",
    "    test.equal(canonicalize_url(source, \"http://a.edu/research/test.html\"), (\"a.edu\", \"/research/test.html\"))\n",
    "    test.equal(canonicalize_url(source, \"/research/test.html\"), (\"source.edu\", \"/research/test.html\"))\n",
    "    test.equal(canonicalize_url(source, \"research/test.html\"), (\"source.edu\", \"/path/segment/research/test.html\"))\n",
    "    test.equal(canonicalize_url(source, \"test.html\"), (\"source.edu\", \"/path/segment/test.html\"))\n",
    "    # You should pretend .. is a folder name:\n",
    "    test.equal(canonicalize_url(source, \"../test.html\"), (\"source.edu\", \"/path/segment/../test.html\"))\n",
    "    test.equal(canonicalize_url(source, \"test.pdf\"), None)\n",
    "    # This clarifies an edge case:\n",
    "    test.equal(canonicalize_url((\"oddbug.edu\", \"\"), \"test.html\"), (\"oddbug.edu\", \"/test.html\"))\n",
    "    test.equal(canonicalize_url((\"oddbug.edu\", \"\"), \"a/b/test.html\"), (\"oddbug.edu\", \"/a/b/test.html\"))\n",
    "    test.equal(canonicalize_url(('www.cs.evenbug.edu', '/evenbug.html'), \"file:///evenbug.html\"), ('www.cs.evenbug.edu', '/evenbug.html'))\n",
    "    test.equal(canonicalize_url((\"evenbug.edu\", \"/a.html\"), \"file:///c.html\"), (\"evenbug.edu\", \"/c.html\"))\n",
    "\n",
    "\n",
    "@test\n",
    "def canonicalize_url(source : Tuple[str, str], target_str : str) -> Optional[Tuple[str, str]]:\n",
    "    \"\"\"Given a source (netloc, path) and a target (netloc, path), construct the canonical target (netloc, path)\n",
    "\n",
    "    args:\n",
    "        source : Tuple[str, str] -- source (netloc, path)\n",
    "        target_str : str -- target url, not canonical\n",
    "\n",
    "    returns : Optional[Tuple[str, str]] -- canonical target (netloc, path) or None if the URL should be ignored\n",
    "    \"\"\"\n",
    "    if(target_str == None):\n",
    "        return None\n",
    "    if(target_str[-5:]!= \".html\"):\n",
    "        return None\n",
    "    source_path = source[1].rsplit(\"/\",1)[0] + \"/\"\n",
    "    parse = urlparse(target_str)\n",
    "    netloc = parse.netloc\n",
    "    path = parse.path\n",
    "    if (netloc == \"\"):\n",
    "        netloc = source[0]\n",
    "#     print(type(path))\n",
    "#     print(len(path))\n",
    "#     print(path)\n",
    "    if(path == None or path == \"\"):\n",
    "        path = source_path + target_str  \n",
    "    elif (path[0] != \"/\"): # incomplete\n",
    "        path = source_path + target_str     \n",
    "    return (netloc, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rwg_map(datum):\n",
    "    \"\"\"populate the (target, source) dict for every hyperlink in the file.\n",
    "\n",
    "    args:\n",
    "        datum : Tuple[str, bytes] -- Tuple of URL and page content\n",
    "\n",
    "    returns : Dict[target, source] -- where\n",
    "        target : Tuple[str, str] -- the netloc, path of the destination webpage\n",
    "        source : YourIntermediateType -- the source webpage, as your chosen type.\n",
    "    \"\"\"\n",
    "    url, html = datum\n",
    "    source = canonicalize_url((\"\",\"\"),url)\n",
    "    dic = collections.defaultdict(set)\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    for link in soup.find_all('a'):\n",
    "        dic[canonicalize_url(source,link.get('href'))].add(url)\n",
    "#     for d in dic:\n",
    "#         dic[d] = list(set(dic[d]))\n",
    "    return dic\n",
    "\n",
    "def rwg_reduce(item):\n",
    "    \"\"\"Restructure the gathered backlinks\n",
    "\n",
    "    args:\n",
    "        item : Tuple[target, sources] -- where\n",
    "            target : Tuple[str, str] -- is the (netloc, path) of the target webpage\n",
    "            sources : List[YourIntermediateType] -- is the list of representations of source webpages\n",
    "\n",
    "    returns: Tuple[key, Dict[incoming_netloc, incoming_path]] -- where\n",
    "        key : Tuple[str, str] -- same as the key type\n",
    "        incoming_netloc : str -- which has the `netloc` of the incoming links\n",
    "        incoming_path : Set[str] -- which is a set of `path`s that follow the associated `netloc`\n",
    "    \"\"\"\n",
    "    target, sources = item\n",
    "#     print(target)\n",
    "#     print(sources)\n",
    "    new_sources = []\n",
    "    dic = collections.defaultdict(set)\n",
    "#     result = collections.defaultdict(list)\n",
    "    for s in sources:\n",
    "        for ss in s:\n",
    "            new_sources.append(ss)\n",
    "    for source in new_sources:\n",
    "        netloc, path = canonicalize_url((\"\",\"\"),source)\n",
    "        dic[netloc].add(path)\n",
    "    return(target, dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('www.cs.byu.edu', '/byu.html'): defaultdict(<class 'set'>, {'www.cs.byu.edu': {'/courses/cs678/syllabus.html', '/courses/cs578/syllabus.html', '/courses/cs142/syllabus.html', '/courses/cs531/syllabus.html', '/courses/cs404/syllabus.html'}, 'iul.cs.byu.edu': {'/morse/morse.html'}, 'osm7.cs.byu.edu': {'/cs240/cs240homepage.html', '/cs327/cs327HomePage.html'}}), ('www.cs.berkeley.edu', '/~russell/aima.html'): defaultdict(<class 'set'>, {'cs.nyu.edu': {'/cs/dept_info/course_home_pages/fall96/G22.2561/index.html', '/cs/dept_info/course_home_pages/spr96/G22.2560/index.html'}, 'www.cs.wisc.edu': {'/~kunen/cs540.html', '/~dyer/cs540.html'}, 'www.cs.indiana.edu': {'/classes/b551/home.html'}}), ('uu-gna.mit.edu:8001', '/uu-gna/text/cc/index.html'): defaultdict(<class 'set'>, {'www.cslab.uky.edu': {'/~jurek/cs420/cs420.html', '/~jurek/cs122/cs122.html'}, 'www.cs.wisc.edu': {'/~cs564-1/cs564.html'}, 'www.cs.cornell.edu': {'/Info/Courses/Current/CS537/course.html'}}), ('www.ncsa.uiuc.edu', '/General/Internet/WWW/HTMLPrimer.html'): defaultdict(<class 'set'>, {'www.cs.rutgers.edu': {'/~dls/index.html', '/~murdocca/index.html'}, 'www.cs.ucsb.edu': {'/~teo/cs130a.html'}, 'www.cs.rochester.edu': {'/urcs.html'}, 'www.cs.wisc.edu': {'/~milo/cs302.html'}}), ('www.cs.umass.edu', '/rcfdocs/newhome/index.html'): defaultdict(<class 'set'>, {'vis-www.cs.umass.edu': {'/~hanson/home.html'}, 'www.cs.umass.edu': {'/~stemple/index.html', '/~mckinley/home.html'}, 'www-ccs.cs.umass.edu': {'/krithi/home.html', '/stankovic/home.html'}, 'dis.cs.umass.edu': {'/lesser.html'}, 'eksl-www.cs.umass.edu': {'/~cohen/home.html'}})}\n",
      "### TESTING reverse_webgraph: PASSED 0/0\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_webpages():\n",
    "    tar = tarfile.open(\"webgraph.tar.gz\", \"r:gz\")\n",
    "    data : List[Tuple[str, bytes]] = []\n",
    "    for member in tar.getmembers():        \n",
    "        num = member.name[:-5].split(\"_\")[-1]\n",
    "        try:\n",
    "            if int(num) < 1000:\n",
    "                f = tar.extractfile(member)\n",
    "                if f is not None:\n",
    "                    # The first line is the URL, all subsequent lines are the data:\n",
    "                    website : str = next(f).decode(\"utf-8\").strip()\n",
    "                    content : bytes = f.read()\n",
    "                    data.append((website, content))\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return data\n",
    "\n",
    "def reverse_webgraph_test(reverse_webgraph):\n",
    "    data = read_webpages()\n",
    "\n",
    "    # Here we use a multiprocessing.Pool, which parallelizes execution over multiple cores.\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        rw = dict(reverse_webgraph(data, pool=pool))\n",
    "\n",
    "    #l = [sorted(((k, len(v)) for k, v in rw.items() if len(v) > 1), key=lambda x: x[1])]\n",
    "    v = {}\n",
    "    for k in [\n",
    "    ('www.cs.byu.edu', '/byu.html'),\n",
    "    ('www.cs.berkeley.edu', '/~russell/aima.html'),\n",
    "    ('uu-gna.mit.edu:8001', '/uu-gna/text/cc/index.html'),\n",
    "    ('www.ncsa.uiuc.edu', '/General/Internet/WWW/HTMLPrimer.html'),\n",
    "    ('www.cs.umass.edu', '/rcfdocs/newhome/index.html')]:\n",
    "        v[k] = rw[k]\n",
    "        \n",
    "    print(v)\n",
    "    \n",
    "@test\n",
    "def reverse_webgraph(data, pool=None):\n",
    "    \"\"\" Count the number of times each character appears in the input data\n",
    "    \n",
    "    args:\n",
    "      data : Tuple[str, List[str]] -- a list of (filename, words)\n",
    "    \"\"\"\n",
    "    return mapreduce(rwg_map, rwg_reduce, data, pool=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank\n",
    "\n",
    "MapReduce can also be used to perform the update step in any iterative computation, such as PageRank. In this assignment we're writing a pagerank that splits the pagerank algorithm into batches of columns.\n",
    "\n",
    "We will try that on the same graphs you may have used back in Homework 2. For your reference, the simple graph is this:\n",
    "```\n",
    "    A -> B -> C\n",
    "    |    ^\n",
    "    v    |\n",
    "    D -> E\n",
    "``` \n",
    "\n",
    "The larger test graph is from Wikipedia.\n",
    "\n",
    "You are allowed to consult online sources as long as all code remains your own. Bear in mind that a lot of online sources involve making PageRank efficient in matrices so large that no single row or column can be held in memory. \n",
    "\n",
    "### Algorithm\n",
    "\n",
    "We have discussed the PageRank algorithm in class; here is some naive pseudocode for calculating it:\n",
    "\n",
    "```python\n",
    "nodes = [...] # A collection of nodes\n",
    "edges = [...] # A collection of edges\n",
    "\n",
    "pr = {...} # Uniform initialization of pagerank\n",
    "d = 0.85   # Smoothing\n",
    "for i in range(iters):\n",
    "    update = {...} # Zeros, one for each node\n",
    "    for source in nodes:\n",
    "        # Count the number of outgoing edges from source\n",
    "        num_outgoing = len(e for e in edges if e is (source -> *))\n",
    "        \n",
    "        # If this is a dead-end node:\n",
    "        if num_outgoing == 0:\n",
    "            for destination in nodes:\n",
    "                update[destination] += pr[source]/len(nodes)\n",
    "        # This node has outgoing connections:\n",
    "        else:\n",
    "            for destination in nodes:\n",
    "                if (source -> destination) in edges:\n",
    "                    update[destination] += pr[source]/num_outgoing\n",
    "\n",
    "    # Update with smoothing:\n",
    "    for node in nodes:\n",
    "        pr[node] = update[node]*d + (1-d)/n\n",
    "```\n",
    "\n",
    "Note that you should not actually use this algorithm for calculating this; that would be too slow. Instead, you should:\n",
    "\n",
    "1. Observe that your function is given `num_outgoing`, the number of outgoing edges from each node, and `incoming`, the source nodes that have edges ending in your nodes.\n",
    "2. Rewrite the above pseudocode to calculate the new pagerank for a single node using `incoming` rather than as shown in the pseudocode above.\n",
    "3. Implement this using PageRank (Additional hint: the reduce function is very simple! All the heavy lifting is done in the map function.)\n",
    "4. Verify that your algorithm is numerically correct on the small, and then the large test case provided.\n",
    "5. Time your algorithm and make sure it completes all tests in under 120s. The Optimization section has hints on this.\n",
    "\n",
    "It may be a good idea to come to TA hours with pseudocode or ideas.\n",
    "\n",
    "### Optimization\n",
    "\n",
    "One of the challenges is making your code run quickly. We've already arranged the code so you should be able to meet the time requirement. Also, we will be evaluating your code on only a few iterations instead of 100 iterations. Ensure that your function takes under 120s on a modern laptop.\n",
    "\n",
    "Here are some hints on optimization:\n",
    "\n",
    "1. While you have sufficient information to solve the problem with the arguments to `pr_map`, you can significantly improve the speed by precomputing a little information. Feel free to change `gen` in `pagerank` to pass precomputed information to the workers.\n",
    "2. It may not be at all necessary to improve this, though, so submit your code once before trying to optimize it. \n",
    "3. We use the generator `gen` to bundle arguments to feed them to `mapreduce`. We are grouping the incoming nodes into chunks of `2**11 = 2048` nodes; we experimentally found that this balances the overhead of transferring data to the worker processes against the amount of work per chunk. You may change this. \n",
    "\n",
    "\n",
    "You will notice that this method is much slower than that in Homework 2. It took about 6s to run all tests using the original sparse matrix implementation on a single core; this will take about a 60s to do the same on a 6-core machine. The main reasons for the slow-down are the setup time (in spawning new processes for the multiprocessing pool), the communication overhead (marshaling, sending data to each process, and unmarshaling it), and the use of the Python dictionary instead of heavily-optimized Numpy sparse matrix math.\n",
    "\n",
    "### Specification\n",
    "\n",
    "You should complete `pr_map` and `pr_reduce` (and optionally modify `pagerank`) to calculate the pagerank of nodes in the input graph, following the type signatures. 100 iterations of this should complete in under 120s. You may choose any intermediate representation (though a very simple one is sufficient to solve the problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to read the edges:\n",
    "def read_graph(basename=\"wikipedia_small\"):\n",
    "    nodes = []\n",
    "    num_outgoing = {}\n",
    "    incoming = {}\n",
    "    with gzip.open(f\"{basename}.nodes.gz\", 'rt', encoding=\"utf-8\", newline=\"\") as f:\n",
    "        for a in f:\n",
    "            a = a.strip()\n",
    "            nodes.append(a)\n",
    "            num_outgoing[a] = 0\n",
    "            incoming[a] = []\n",
    "\n",
    "    with gzip.open(f\"{basename}.graph.gz\", 'rt', encoding=\"utf-8\", newline=\"\") as f:\n",
    "        links = []\n",
    "        for row in f:\n",
    "            i, j = tuple(row.strip().split())\n",
    "            num_outgoing[nodes[int(i)]] += 1\n",
    "            incoming[nodes[int(j)]].append(nodes[int(i)])\n",
    "    return num_outgoing, incoming\n",
    "\n",
    "# Utility function to help group the input into chunks:\n",
    "# https://docs.python.org/3/library/itertools.html\n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(fillvalue=fillvalue, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank_test(pagerank):\n",
    "    G_num_outgoing = {\"A\": 2, \"B\": 1, \"C\": 0, \"D\": 1, \"E\": 1}\n",
    "    G_incoming = { \"A\": set(), \"B\": set(\"AE\"), \"C\": {\"B\"}, \"D\": {\"A\"}, \"E\": {\"D\"} }\n",
    "\n",
    "    pr = pagerank(G_num_outgoing, G_incoming)    \n",
    "    test.true(abs(pr['A']-0.08510862387068166) < 1e-7)\n",
    "    test.true(abs(pr['B']-0.28124676686965944) < 1e-7)\n",
    "    test.true(abs(pr['C']-0.3241683757098922) < 1e-7)\n",
    "    test.true(abs(pr['D']-0.12127978901572137) < 1e-7)\n",
    "    test.true(abs(pr['E']-0.18819644453404483) < 1e-7)\n",
    "    \n",
    "    # Remove this line to run the rest of the tests:\n",
    "#     GW_num_outgoing, GW_incoming = read_graph()\n",
    "    \n",
    "#     with multiprocessing.Pool() as pool:\n",
    "#         start_time = time.perf_counter()\n",
    "#         pr = pagerank(GW_num_outgoing, GW_incoming, pool=pool, iters=100)\n",
    "#         duration = time.perf_counter() - start_time\n",
    "        \n",
    "#     test.equal(len(pr), 24166)\n",
    "\n",
    "#     # Numerical check\n",
    "#     test.true(abs(pr['United_States'] - 0.00275188705264) < 1e-5)\n",
    "#     test.true(abs(pr['2008']          - 0.00217342514773) < 1e-5)\n",
    "#     test.true(abs(pr['Canada']        - 0.00109896195215) < 1e-5)\n",
    "#     test.true(abs(pr['World_War_II']  - 0.00104913079624) < 1e-5)\n",
    "#     test.true(abs(pr['List_of_African_films'] - 0.00100713870383) < 1e-5)\n",
    "#     test.true(abs(pr['Europe']        - 0.000937690025073) < 1e-5)\n",
    "#     test.true(abs(pr['English_language'] - 0.000908144359626) < 1e-5)\n",
    "#     test.true(abs(pr['Geographic_coordinate_system'] - 0.000891711151403) < 1e-5)\n",
    "#     test.true(abs(pr['Latin']         - 0.000888662228804) < 1e-5)\n",
    "\n",
    "#     print(f\"Completed pagerank in {round(duration, 1)}s\")\n",
    "#     test.true(duration < 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING pagerank: PASSED 5/5\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pr_map(inp):\n",
    "    \"\"\"PageRank map function\n",
    "    \n",
    "    args:\n",
    "        inp: Tuple[entries, pr, num_outgoing, d,...] -- where\n",
    "            entries : List[Tuple[node, incoming]] -- a list of nodes to process, where\n",
    "                node : str -- the node id\n",
    "                incoming : Collection[str] -- the node ids that have an edge to this node\n",
    "            pr : Dict[str, float] -- from node id to pagerank at the start of the iteration\n",
    "            num_outgoing : Dict[str, int] -- from node id to the number of edges leaving that node\n",
    "            d : float -- the damping factor\n",
    "            ...additional entries to speed up your code\n",
    "    \n",
    "    returns : Dict[str, Intermediate], where the key is the node id and Intermediate is whatever your\n",
    "            intermediate step representation is. Each entry in entries must be represented in your output\n",
    "    \"\"\"\n",
    "\n",
    "    entries, pr, num_outgoing, d, update = inp\n",
    "    rv = {}\n",
    "    \n",
    "    for entry in entries:#(('A', set()), ('B', {'A', 'E'}), ('C', {'B'}), ('D', {'A'}), ('E', {'D'})\n",
    "        if(entry != None):\n",
    "            dest = entry[0]\n",
    "            sources = entry[1]\n",
    "\n",
    "            for source in sources:\n",
    "                update[dest] += pr[source]/num_outgoing[source]\n",
    "\n",
    "    # We remove null entries created by grouper:\n",
    "    for node, incoming_links in filter(lambda x: x, entries):\n",
    "        rv[node] = update[node]*d + (1-d)/len(num_outgoing) # Your intermediate representation here\n",
    "    return rv\n",
    "\n",
    "def pr_reduce(inp):\n",
    "    \"\"\"PageRank reduce function\n",
    "    \n",
    "    args:\n",
    "        inp : Tuple[str, List[Intermediate]]\n",
    "        \n",
    "    returns: Tuple[str, float] -- the node id and its weight at the end of this iteration\n",
    "    \"\"\"\n",
    "#     print(\"reduce\")\n",
    "    node, list_of_reps = inp\n",
    "#     print(node)\n",
    "#     print(list_of_reps)\n",
    "    return node, list_of_reps[0]\n",
    "\n",
    "@test\n",
    "def pagerank(num_outgoing, incoming, d=0.85, iters=100, pool=None):\n",
    "    \"\"\" Compute the PageRank score for each node in the network using a MapReduce version of the power method\n",
    "\n",
    "    args:\n",
    "        num_outgoing: Dict[str, int] -- the number of outgoing links from each node\n",
    "        incoming: Dict[str, Set[str]] -- the set of pages that link to each node\n",
    "\n",
    "    kwargs:\n",
    "        d : float -- damping factor (note that this is defined the same as homework 2)\n",
    "        iters : int -- number of iterations\n",
    "\n",
    "    returns: Dict[str, int] -- the importance score for each node\n",
    "    \"\"\"\n",
    "    n = len(num_outgoing)\n",
    "    # Initialize the pagerank as a uniform distribution. You should not change this:\n",
    "    pr = {k : 1./n for k in num_outgoing.keys()}\n",
    "    \n",
    "    \n",
    "    for i in range(iters):\n",
    "        update = {}\n",
    "        count = 0\n",
    "        for node in num_outgoing:\n",
    "            if(num_outgoing[node] == 0):\n",
    "                count += pr[node]/len(num_outgoing)\n",
    "#     print(num_outgoing)\n",
    "        for node in num_outgoing:\n",
    "            update[node] = count\n",
    "#         print(update)\n",
    "        gen = ((entries, pr, num_outgoing, d, update) for entries in grouper(incoming.items(), 2 ** 11))\n",
    "        pr = dict(mapreduce(pr_map, pr_reduce, gen, pool=None))\n",
    "    return pr"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
